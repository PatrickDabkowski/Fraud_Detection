{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806d2492-1ffa-48ee-85df-bb0cde9ebd10",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbb1b6-9ac5-4287-8917-47c5830e11f4",
   "metadata": {},
   "source": [
    "## Task definition:\n",
    "Our aim is to define model that will work with unbalanced data, what can we achieve by Deep Learning methods with the proper metrics and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baf4a94-c1ac-477d-aa17-d1b709545d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  True\n",
      "GPU built:  True\n",
      "device:  mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print('GPU: ', torch.backends.mps.is_available()) \n",
    "print('GPU built: ', torch.backends.mps.is_built())\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718d2042-984b-43fb-a36b-abb03d00de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function faster than pd.dummies\n",
    "def transofrm(data):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == 'object':  \n",
    "            data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876be7b3-4f45-491e-8c2e-b793c3afb6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt',\n",
       "       'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat',\n",
       "       'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat',\n",
       "       'merch_long', 'is_fraud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('fraudTrain.csv')\n",
    "df_test = pd.read_csv('fraudTest.csv')\n",
    "df_train = df_train.drop(columns='Unnamed: 0')\n",
    "df_test = df_test.drop(columns='Unnamed: 0')\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda6bf4d-0176-44e9-ab95-5b0856801ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features number:  21\n"
     ]
    }
   ],
   "source": [
    "df_train = transofrm(df_train)\n",
    "train_features = df_train.drop(columns=['is_fraud']).values\n",
    "print('features number: ', train_features.shape[1])\n",
    "train_labels = df_train['is_fraud'].values\n",
    "\n",
    "df_test= transofrm(df_test)\n",
    "test_features = df_train.drop(columns=['is_fraud']).values\n",
    "test_labels = df_train['is_fraud'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b4e1d-698e-4faa-9faa-bb39f5c0f871",
   "metadata": {},
   "source": [
    "## Deep Learing Metrics\n",
    "\n",
    "-Sensitivity and Specificity are better for imbalanced data\n",
    "\n",
    "Especially crucial is sensitivity because it measures correctly classified positive samples with respect to the total number of positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33f70c1-8c63-4963-a977-4384fb9504b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = df_train['is_fraud'].value_counts()\n",
    "\n",
    "possitive_class = count_classes[1]\n",
    "negative_class = count_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558ff26-0846-4258-9b00-ca33a17171c5",
   "metadata": {},
   "source": [
    "Positive class is much fewer represented that's why the model supposed to pay more attention to it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b43896-e260-4a7c-8873-51b5cd8f556a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "positive_class_weight = int(count_classes[0] / count_classes[1])\n",
    "negative_class_weight = 1\n",
    "\n",
    "print(positive_class_weight)\n",
    "print(negative_class_weight)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(positive_class_weight)).to(device) \n",
    "# pos_weight=torch.Tensor([positive_class_weight, negative_class_weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc2128-2a7f-455a-99e2-2ac9a9f809b5",
   "metadata": {},
   "source": [
    "## ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceb0427-1daf-4c4c-802b-8135dad8a84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=21, out_features=42, bias=True)\n",
      "  (lin2): Linear(in_features=42, out_features=21, bias=True)\n",
      "  (lin3): Linear(in_features=21, out_features=10, bias=True)\n",
      "  (lin4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_features, output_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = nn.Linear(input_features, input_features*2)\n",
    "        self.lin2 = nn.Linear(input_features*2, input_features)\n",
    "        self.lin3 = nn.Linear(input_features, input_features // 2)\n",
    "        self.lin4 = nn.Linear(input_features // 2, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MLP(train_features.shape[1], 1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3fc3b-d57d-4096-beb8-d14c6372a301",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f1826c-e1e9-4797-96a9-991947aeca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541d662-ce7d-48fb-bd97-a233fccf4548",
   "metadata": {},
   "source": [
    "## Training\n",
    "During training we can see how not trustworthy accuracy is for unbalanced data, after one epoch (with Sensitivity equal to 0 and huge loss) we achieve over 99% accuracy. That's why it is important to be aware of distributions that the dataset follows and properties measured by our metrics. \n",
    "That example shows also how ineffective could be early on the test set results, even if they don't improve much, we see a huge improvement in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637c7e4-5f83-4502-a6bf-10c2ae920475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 568264321588.1188, Test Sensitivity: 0.0, Test Accuracy: 0.9942113482561166\n",
      "Epoch: 002, Loss: 229926043107.41568, Test Sensitivity: 0.0, Test Accuracy: 0.9942113482561166\n",
      "Epoch: 003, Loss: 10021040342.686415, Test Sensitivity: 0.0, Test Accuracy: 0.9942113482561166\n",
      "Epoch: 004, Loss: 0.02149232729539414, Test Sensitivity: 0.0, Test Accuracy: 0.9942113482561166\n",
      "Epoch: 005, Loss: 0.021489179444464282, Test Sensitivity: 0.0, Test Accuracy: 0.9942113482561166\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)  \n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    true_positives = 0\n",
    "    actual_positives = 0\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        features, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)  \n",
    "        out = model(features)\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        true_positives += (pred == 1).logical_and(labels == 1).sum().item()\n",
    "        actual_positives += labels.sum().item()\n",
    "        correct_predictions += (pred == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "    sensitivity = true_positives / actual_positives if actual_positives > 0 else None\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else None\n",
    "    \n",
    "    return sensitivity, accuracy\n",
    "\n",
    "num_epochs = 30\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    sensitivity, test_acc = test(model, test_loader, device)\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss}, Test Sensitivity: {sensitivity}, Test Accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
